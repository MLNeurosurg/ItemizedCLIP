wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2025-12-05,11:29:52 | INFO | Use custom clip loss.
2025-12-05,11:29:52 | INFO | Start epoch 0
2025-12-05,11:30:28 | INFO | Train Epoch: 0 [   256/220001 (0%)] Data (t): 10.858 Batch (t): 35.723, 1.79154/s, 0.223943/s/gpu LR: 0.000000 Logit Scale: 14.286 Contrastive_loss: 78.614 (78.614) Loss: 78.614 (78.614)
[rank0]:I1205 11:30:28.314477 173020 site-packages/torch/nn/parallel/distributed.py:1529] Reducer buckets have been rebuilt in this iteration.
2025-12-05,11:44:18 | INFO | Train Epoch: 0 [ 25856/220001 (12%)] Data (t): 0.004 Batch (t): 8.302, 7.77617/s, 0.972021/s/gpu LR: 0.000009 Logit Scale: 14.287 Contrastive_loss: 44.229 (61.421) Loss: 44.229 (61.421)
2025-12-05,11:58:06 | INFO | Train Epoch: 0 [ 51456/220001 (23%)] Data (t): 0.002 Batch (t): 8.285, 7.76871/s, 0.971089/s/gpu LR: 0.000018 Logit Scale: 14.287 Contrastive_loss: 40.442 (54.428) Loss: 40.442 (54.428)
2025-12-05,12:11:54 | INFO | Train Epoch: 0 [ 77056/220001 (35%)] Data (t): 0.002 Batch (t): 8.275, 7.72736/s, 0.965921/s/gpu LR: 0.000026 Logit Scale: 14.287 Contrastive_loss: 39.896 (50.795) Loss: 39.896 (50.795)
2025-12-05,12:25:43 | INFO | Train Epoch: 0 [102656/220001 (47%)] Data (t): 0.002 Batch (t): 8.293, 7.76656/s, 0.970820/s/gpu LR: 0.000035 Logit Scale: 14.288 Contrastive_loss: 38.895 (48.415) Loss: 38.895 (48.415)
2025-12-05,12:39:31 | INFO | Train Epoch: 0 [128256/220001 (58%)] Data (t): 0.002 Batch (t): 8.278, 7.74389/s, 0.967986/s/gpu LR: 0.000044 Logit Scale: 14.289 Contrastive_loss: 31.163 (45.540) Loss: 31.163 (45.540)
2025-12-05,12:53:20 | INFO | Train Epoch: 0 [153856/220001 (70%)] Data (t): 0.002 Batch (t): 8.291, 7.74960/s, 0.968700/s/gpu LR: 0.000053 Logit Scale: 14.290 Contrastive_loss: 34.957 (44.028) Loss: 34.957 (44.028)
2025-12-05,13:07:07 | INFO | Train Epoch: 0 [179456/220001 (82%)] Data (t): 0.002 Batch (t): 8.273, 7.72540/s, 0.965675/s/gpu LR: 0.000061 Logit Scale: 14.289 Contrastive_loss: 33.761 (42.745) Loss: 33.761 (42.745)
2025-12-05,13:20:56 | INFO | Train Epoch: 0 [205056/220001 (93%)] Data (t): 0.002 Batch (t): 8.286, 7.73033/s, 0.966291/s/gpu LR: 0.000070 Logit Scale: 14.288 Contrastive_loss: 36.126 (42.009) Loss: 36.126 (42.009)
2025-12-05,13:28:56 | INFO | Train Epoch: 0 [219904/220001 (100%)] Data (t): 0.002 Batch (t): 8.279, 7.76854/s, 0.971067/s/gpu LR: 0.000075 Logit Scale: 14.288 Contrastive_loss: 34.107 (41.219) Loss: 34.107 (41.219)
2025-12-05,13:29:23 | INFO | Eval Epoch: 1 TCSimval/TCSim_Rank_All_Entry: 398.6639	TCSimval/TCSim_Rank_Worst_Entry: 625.9163	TCSimval/TCSimRankT2i@1: 0.0250	TCSimval/TCSimRankT2I@10: 0.1718	TCSimval/TCSimRankT2I@100: 0.6259	TCSimval/TCSimRankI2T@1: 0.0252	TCSimval/TCSimRankI2T@10: 0.1694	TCSimval/TCSimRankI2T@100: 0.5403	TCSimval/TCSimRankI2T_COMPLETE@10: 0.0202	TCSimval/TCSimRankI2T_COMPLETE@100: 0.1411	TCSimval/TCSimRankI2T_COMPLETE@1000: 0.7792
2025-12-05,13:29:34 | INFO | Start epoch 1
2025-12-05,13:29:55 | INFO | Train Epoch: 1 [   256/220001 (0%)] Data (t): 11.439 Batch (t): 20.168, 3.17337/s, 0.396671/s/gpu LR: 0.000075 Logit Scale: 14.288 Contrastive_loss: 30.330 (30.330) Loss: 30.330 (30.330)
2025-12-05,13:43:41 | INFO | Train Epoch: 1 [ 25856/220001 (12%)] Data (t): 0.002 Batch (t): 8.267, 7.73417/s, 0.966772/s/gpu LR: 0.000084 Logit Scale: 14.288 Contrastive_loss: 30.907 (30.618) Loss: 30.907 (30.618)
2025-12-05,13:57:29 | INFO | Train Epoch: 1 [ 51456/220001 (23%)] Data (t): 0.002 Batch (t): 8.281, 7.73855/s, 0.967318/s/gpu LR: 0.000093 Logit Scale: 14.289 Contrastive_loss: 33.387 (31.541) Loss: 33.387 (31.541)
2025-12-05,14:11:17 | INFO | Train Epoch: 1 [ 77056/220001 (35%)] Data (t): 0.002 Batch (t): 8.276, 7.74420/s, 0.968024/s/gpu LR: 0.000101 Logit Scale: 14.290 Contrastive_loss: 28.030 (30.663) Loss: 28.030 (30.663)
2025-12-05,14:25:05 | INFO | Train Epoch: 1 [102656/220001 (47%)] Data (t): 0.002 Batch (t): 8.282, 7.78328/s, 0.972911/s/gpu LR: 0.000110 Logit Scale: 14.291 Contrastive_loss: 31.732 (30.877) Loss: 31.732 (30.877)
2025-12-05,14:38:52 | INFO | Train Epoch: 1 [128256/220001 (58%)] Data (t): 0.002 Batch (t): 8.272, 7.74624/s, 0.968280/s/gpu LR: 0.000119 Logit Scale: 14.293 Contrastive_loss: 31.030 (30.903) Loss: 31.030 (30.903)
2025-12-05,14:52:40 | INFO | Train Epoch: 1 [153856/220001 (70%)] Data (t): 0.002 Batch (t): 8.275, 7.70459/s, 0.963074/s/gpu LR: 0.000128 Logit Scale: 14.296 Contrastive_loss: 29.732 (30.735) Loss: 29.732 (30.735)
2025-12-05,15:06:27 | INFO | Train Epoch: 1 [179456/220001 (82%)] Data (t): 0.002 Batch (t): 8.272, 7.73300/s, 0.966625/s/gpu LR: 0.000137 Logit Scale: 14.303 Contrastive_loss: 28.390 (30.442) Loss: 28.390 (30.442)
2025-12-05,15:20:13 | INFO | Train Epoch: 1 [205056/220001 (93%)] Data (t): 0.002 Batch (t): 8.263, 7.73862/s, 0.967327/s/gpu LR: 0.000145 Logit Scale: 14.306 Contrastive_loss: 25.888 (29.936) Loss: 25.888 (29.936)
2025-12-05,15:28:13 | INFO | Train Epoch: 1 [219904/220001 (100%)] Data (t): 0.002 Batch (t): 8.277, 7.80957/s, 0.976196/s/gpu LR: 0.000150 Logit Scale: 14.309 Contrastive_loss: 30.975 (30.040) Loss: 30.975 (30.040)
2025-12-05,15:28:35 | INFO | Eval Epoch: 2 TCSimval/TCSim_Rank_All_Entry: 244.3392	TCSimval/TCSim_Rank_Worst_Entry: 427.1724	TCSimval/TCSimRankT2i@1: 0.0644	TCSimval/TCSimRankT2I@10: 0.3031	TCSimval/TCSimRankT2I@100: 0.7734	TCSimval/TCSimRankI2T@1: 0.0696	TCSimval/TCSimRankI2T@10: 0.3327	TCSimval/TCSimRankI2T@100: 0.7238	TCSimval/TCSimRankI2T_COMPLETE@10: 0.0444	TCSimval/TCSimRankI2T_COMPLETE@100: 0.2641	TCSimval/TCSimRankI2T_COMPLETE@1000: 0.8800
2025-12-05,15:28:47 | INFO | Start epoch 2
2025-12-05,15:29:05 | INFO | Train Epoch: 2 [   256/220001 (0%)] Data (t): 10.341 Batch (t): 18.425, 3.47349/s, 0.434186/s/gpu LR: 0.000150 Logit Scale: 14.309 Contrastive_loss: 25.618 (25.618) Loss: 25.618 (25.618)
2025-12-05,15:42:52 | INFO | Train Epoch: 2 [ 25856/220001 (12%)] Data (t): 0.002 Batch (t): 8.273, 7.78929/s, 0.973661/s/gpu LR: 0.000159 Logit Scale: 14.312 Contrastive_loss: 23.821 (24.719) Loss: 23.821 (24.719)
2025-12-05,15:56:40 | INFO | Train Epoch: 2 [ 51456/220001 (23%)] Data (t): 0.002 Batch (t): 8.277, 7.71497/s, 0.964371/s/gpu LR: 0.000168 Logit Scale: 14.316 Contrastive_loss: 29.256 (26.232) Loss: 29.256 (26.232)
2025-12-05,16:10:28 | INFO | Train Epoch: 2 [ 77056/220001 (35%)] Data (t): 0.002 Batch (t): 8.276, 7.73110/s, 0.966387/s/gpu LR: 0.000175 Logit Scale: 14.320 Contrastive_loss: 26.117 (26.203) Loss: 26.117 (26.203)
2025-12-05,16:24:16 | INFO | Train Epoch: 2 [102656/220001 (47%)] Data (t): 0.002 Batch (t): 8.279, 7.74440/s, 0.968050/s/gpu LR: 0.000175 Logit Scale: 14.324 Contrastive_loss: 30.013 (26.965) Loss: 30.013 (26.965)
2025-12-05,16:38:04 | INFO | Train Epoch: 2 [128256/220001 (58%)] Data (t): 0.002 Batch (t): 8.280, 7.71474/s, 0.964342/s/gpu LR: 0.000175 Logit Scale: 14.327 Contrastive_loss: 26.793 (26.936) Loss: 26.793 (26.936)
2025-12-05,16:51:52 | INFO | Train Epoch: 2 [153856/220001 (70%)] Data (t): 0.002 Batch (t): 8.287, 7.70950/s, 0.963687/s/gpu LR: 0.000175 Logit Scale: 14.332 Contrastive_loss: 24.974 (26.656) Loss: 24.974 (26.656)
